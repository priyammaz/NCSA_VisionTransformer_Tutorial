# Hal Training: Vision Transformers
This repository contains all the code presented for the Vision Transformer Tutorial for the National Center for Supercomputing Applications HAL Tutorials. In this exploration our goal is to build a Vision Transformer from scratch!

Concepts covered:
- Why Attention and not Convolutions?
- Patch Embeddings (Conversion of Images to "sequences")
- Understanding CLS tokens and Positional Embeddings
- How to build a single Attention Head (Q, K, V)
- Expanding Single Headed Attention to MultiHeaded Attention
- The purpose of LayerNormalization
